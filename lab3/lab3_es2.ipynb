{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_apriori(matrix, minsup):\n",
    "    \n",
    "    def prune_itemsets(candidates, previous_frequent, minsup, supports):\n",
    "        # 1st pruning: remove all the itemsets containing one or more infrequent subsets\n",
    "        to_remove = []\n",
    "        for i in range(len(candidates)):\n",
    "            for j in range(len(candidates[i])):\n",
    "                subset = [candidates[i][k] for k in range(len(candidates[i])) if k != j]\n",
    "                if(subset not in previous_frequent):\n",
    "                    to_remove.append(i)\n",
    "                    break\n",
    "        candidates = [candidates[i] for i in range(len(candidates)) if i not in to_remove]\n",
    "        \n",
    "        # 2nd pruning: remove all itemsets whose support < minsup\n",
    "        to_remove = []\n",
    "        for i in range(len(candidates)):\n",
    "            combined_array = np.array([1 for i in range(matrix.shape[0])])\n",
    "            for j in range(len(candidates[i])):\n",
    "                combined_array = combined_array & matrix[:,candidates[i][j]]\n",
    "            support = sum(combined_array)\n",
    "            if(support < minsup):\n",
    "                to_remove.append(i)\n",
    "            else:\n",
    "                supports += [([candidates[i][j] for j in range(len(candidates[i]))], support/matrix.shape[0])]\n",
    "        candidates = [candidates[i] for i in range(len(candidates)) if i not in to_remove]\n",
    "        \n",
    "        return candidates\n",
    "\n",
    "    def generate_couples(v):\n",
    "        couples = []\n",
    "        for i in range(len(v)):\n",
    "            for j in range(i + 1, len(v)):\n",
    "                couples.append([v[i][0], v[j][0]])\n",
    "        return couples \n",
    "\n",
    "    def generate_candidates(frequent, k):\n",
    "        candidates = []\n",
    "        for i in range(len(frequent)):\n",
    "            for j in range(i + 1,len(frequent)):\n",
    "                if frequent[i][:k-1] == frequent_itemsets[j][:k-1]:\n",
    "                    candidates.append(frequent[i][:k] + [frequent[j][k - 1]])\n",
    "        return candidates\n",
    "\n",
    "    \n",
    "    minsup = minsup*matrix.shape[0]\n",
    "    supports = []\n",
    "    frequent_itemsets = [[i] for i in range(matrix.shape[1]) if sum(matrix[:,i]) >= minsup]\n",
    "    supports += [(i, sum(matrix[:,i[0]])/matrix.shape[0]) for i in frequent_itemsets]\n",
    "    \n",
    "    k = 1\n",
    "    while frequent_itemsets != [] and k < matrix.shape[1]:\n",
    "        if k == 1:\n",
    "            candidate_itemsets = generate_couples(frequent_itemsets)\n",
    "        else:\n",
    "            candidate_itemsets = generate_candidates(frequent_itemsets, k)               \n",
    "        frequent_itemsets = prune_itemsets(candidate_itemsets, frequent_itemsets, minsup, supports)\n",
    "        k += 1\n",
    "        \n",
    "    supports.sort(key= lambda tup: tup[1], reverse = True)\n",
    "    return supports\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"modified_coco.json\") as f: \n",
    "    obj = json.load(f)\n",
    "\n",
    "transactions = []\n",
    "distinct_items = []\n",
    "for i in range(len(obj)):\n",
    "    new_transaction = list(set(obj[i][\"annotations\"]))\n",
    "    transactions.append(new_transaction)\n",
    "    for j in range(len(new_transaction)):\n",
    "        if new_transaction[j] not in distinct_items:\n",
    "            distinct_items.append(new_transaction[j])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import timeit\n",
    "\n",
    "matrix = []\n",
    "for i in range(len(transactions)):\n",
    "    presence_vector = [(x in transactions[i]) for x in distinct_items]\n",
    "    matrix.append(presence_vector)\n",
    "matrix = np.array(matrix)    \n",
    "\n",
    "df = pd.DataFrame(data=matrix, columns=distinct_items)\n",
    "minsup = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_supports = my_apriori(matrix, minsup)\n",
    "supports = apriori(df, min_support=minsup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation is 99.46 times slower than the pre-implemented one.\n"
     ]
    }
   ],
   "source": [
    "t1 = timeit.timeit(lambda: my_apriori(matrix, minsup), number = 1)\n",
    "t2 = timeit.timeit(lambda: apriori(df, min_support=minsup), number = 1) \n",
    "\n",
    "time_comp = t1/t2\n",
    "\n",
    "if time_comp >= 1:\n",
    "    print(f\"Your implementation is {time_comp:.2f} times slower than the pre-implemented one.\")\n",
    "else:\n",
    "    print(f\"Your implementation is {1/time_comp:.2f} times faster than the pre-implemented one.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsupport\titemsets\n",
      "0\t0.5886\t['person']\n",
      "1\t0.4338\t['bench']\n",
      "2\t0.3704\t['car']\n",
      "3\t0.323\t['traffic light']\n",
      "4\t0.3208\t['bench', 'person']\n",
      "5\t0.2386\t['car', 'person']\n",
      "6\t0.1978\t['car', 'traffic light']\n",
      "7\t0.1902\t['person', 'traffic light']\n",
      "8\t0.1362\t['car', 'person', 'traffic light']\n",
      "9\t0.1346\t['fire hydrant']\n",
      "10\t0.1332\t['stop sign']\n",
      "11\t0.1286\t['truck']\n",
      "12\t0.123\t['handbag']\n",
      "13\t0.1224\t['person', 'handbag']\n",
      "14\t0.1032\t['car', 'truck']\n",
      "15\t0.0912\t['bus']\n",
      "16\t0.0852\t['backpack']\n",
      "17\t0.0828\t['person', 'truck']\n",
      "18\t0.0826\t['person', 'backpack']\n",
      "19\t0.0778\t['truck', 'traffic light']\n",
      "20\t0.0762\t['bicycle']\n",
      "21\t0.0762\t['person', 'bus']\n",
      "22\t0.0712\t['car', 'bench']\n",
      "23\t0.0668\t['bus', 'traffic light']\n",
      "24\t0.0666\t['person', 'bicycle']\n",
      "25\t0.066\t['car', 'truck', 'traffic light']\n",
      "26\t0.0656\t['car', 'bus']\n",
      "27\t0.0656\t['car', 'person', 'truck']\n",
      "28\t0.0622\t['car', 'fire hydrant']\n",
      "29\t0.0604\t['car', 'bench', 'person']\n",
      "30\t0.0602\t['chair']\n",
      "31\t0.0584\t['bench', 'handbag']\n",
      "32\t0.058\t['bench', 'person', 'handbag']\n",
      "33\t0.0576\t['stop sign', 'car']\n",
      "34\t0.0574\t['car', 'handbag']\n",
      "35\t0.0572\t['car', 'person', 'handbag']\n",
      "36\t0.0558\t['person', 'bus', 'traffic light']\n",
      "37\t0.0554\t['parking meter']\n",
      "38\t0.0544\t['car', 'person', 'bus']\n",
      "39\t0.0538\t['person', 'fire hydrant']\n",
      "40\t0.0526\t['car', 'bus', 'traffic light']\n",
      "41\t0.052\t['person', 'truck', 'traffic light']\n",
      "42\t0.0518\t['traffic light', 'handbag']\n",
      "43\t0.0518\t['person', 'traffic light', 'handbag']\n",
      "44\t0.0506\t['bench', 'chair']\n",
      "    support     itemsets\n",
      "0    0.1332          (0)\n",
      "1    0.3704          (1)\n",
      "2    0.4338          (3)\n",
      "3    0.0602          (4)\n",
      "4    0.5886          (6)\n",
      "5    0.1346          (8)\n",
      "6    0.0762          (9)\n",
      "7    0.0912         (10)\n",
      "8    0.0852         (11)\n",
      "9    0.0554         (12)\n",
      "10   0.1286         (13)\n",
      "11   0.3230         (17)\n",
      "12   0.1230         (22)\n",
      "13   0.0576       (0, 1)\n",
      "14   0.0712       (1, 3)\n",
      "15   0.2386       (1, 6)\n",
      "16   0.0622       (8, 1)\n",
      "17   0.0656      (1, 10)\n",
      "18   0.1032      (1, 13)\n",
      "19   0.1978      (1, 17)\n",
      "20   0.0574      (1, 22)\n",
      "21   0.0506       (3, 4)\n",
      "22   0.3208       (3, 6)\n",
      "23   0.0584      (3, 22)\n",
      "24   0.0538       (8, 6)\n",
      "25   0.0666       (9, 6)\n",
      "26   0.0762      (10, 6)\n",
      "27   0.0826      (11, 6)\n",
      "28   0.0828      (13, 6)\n",
      "29   0.1902      (17, 6)\n",
      "30   0.1224      (6, 22)\n",
      "31   0.0668     (17, 10)\n",
      "32   0.0778     (17, 13)\n",
      "33   0.0518     (17, 22)\n",
      "34   0.0604    (1, 3, 6)\n",
      "35   0.0544   (1, 10, 6)\n",
      "36   0.0656   (1, 13, 6)\n",
      "37   0.1362   (1, 6, 17)\n",
      "38   0.0572   (1, 6, 22)\n",
      "39   0.0526  (1, 10, 17)\n",
      "40   0.0660  (1, 13, 17)\n",
      "41   0.0580   (3, 6, 22)\n",
      "42   0.0558  (17, 10, 6)\n",
      "43   0.0520  (17, 13, 6)\n",
      "44   0.0518  (17, 6, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\tsupport\\titemsets\")\n",
    "for i in range(len(my_supports)):\n",
    "    print(f\"{i}\\t{my_supports[i][1]}\\t{[distinct_items[j] for j in my_supports[i][0]]}\")\n",
    "\n",
    "print(supports)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
